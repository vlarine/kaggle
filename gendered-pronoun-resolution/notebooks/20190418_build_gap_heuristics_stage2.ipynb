{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAP heuristics\n",
    "\n",
    "Taken \"as is\" from this [Public Kernel](https://www.kaggle.com/sattree/2-reproducing-gap-results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these if needed\n",
    "!head ../input/test_stage_2.tsv > ../input/toy_train.tsv \n",
    "PATH_TO_TRAIN = '../input/toy_train.tsv' # dummy file, just in order not to change the code below\n",
    "PATH_TO_TEST = '../input/test_stage_2.tsv'\n",
    "PATH_OUT_TRAIN_FEAT = '../features/toy_train_gap_heuristics.tsv'\n",
    "PATH_OUT_TEST_FEAT = '../features/test_gap_heuristics.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download necessary models and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 12.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download and install all dependencies\n",
    "# gpr_pub contains the heuristics models and supplementary code\n",
    "#!git clone https://github.com/sattree/gpr_pub.git\n",
    "#!wget -P /home/kashn500/heavy_models/ http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
    "#!unzip /home/kashn500/heavy_models/stanford-corenlp-full-2018-10-05.zip\n",
    "#!pip install allennlp --ignore-installed greenlet\n",
    "#!pip install attrdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, classification_report\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.models.archival import load_archive\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from gpr_pub.utils import CoreNLPServer\n",
    "\n",
    "# gap_scorer_ext has minor fixes for py3 and to take pandas df as input instead of filepaths\n",
    "from gpr_pub.gap.gap_scorer_ext import read_annotations, calculate_scores, add_to_score_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic models implement coref resolution based on heuristics described in the paper\n",
    "# Pronoun resolution is a simple wrapper to convert coref predictions into class-specific labels\n",
    "# Multi pass sieve model implements backoff mechanism\n",
    "from gpr_pub.models.heuristics.random_distance import RandomModel\n",
    "from gpr_pub.models.heuristics.token_distance import TokenDistanceModel\n",
    "from gpr_pub.models.heuristics.syntactic_distance import StanfordSyntacticDistanceModel\n",
    "from gpr_pub.models.heuristics.parallelism import AllenNLPParallelismModel as ParallelismModel\n",
    "from gpr_pub.models.heuristics.url_title import StanfordURLTitleModel as URLModel\n",
    "\n",
    "from gpr_pub.models.pronoun_resolution import PronounResolutionModel\n",
    "\n",
    "from gpr_pub.models.multi_pass_sieve import MultiPassSieveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate stanford corenlp server\n",
    "STANFORD_CORENLP_PATH = 'stanford-corenlp-full-2018-10-05/'\n",
    "server = CoreNLPServer(classpath=STANFORD_CORENLP_PATH,\n",
    "                        corenlp_options=AttrDict({'port': 9090, \n",
    "                                                  'timeout': '600000', \n",
    "                                                  'quiet': 'true',\n",
    "                                                  'preload': 'tokenize,spplit,lemma,parse,deparse'}))\n",
    "server.start()\n",
    "STANFORD_SERVER_URL = server.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cymem==1.31.2 spacy==2.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: .*weight_ih.*\n",
      "Did not use initialization regex that was passed: .*bias_hh.*\n",
      "Did not use initialization regex that was passed: .*bias_ih.*\n",
      "Did not use initialization regex that was passed: .*weight_hh.*\n"
     ]
    }
   ],
   "source": [
    "# Instantiate base models\n",
    "STANFORD_MODEL = CoreNLPParser(url=STANFORD_SERVER_URL)\n",
    "SPACY_MODEL = spacy.load('en_core_web_lg')\n",
    "model_url = 'https://s3-us-west-2.amazonaws.com/allennlp/models/biaffine-dependency-parser-ptb-2018.08.23.tar.gz'\n",
    "archive = load_archive(model_url, cuda_device=1)\n",
    "ALLEN_DEP_MODEL = Predictor.from_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate heuristic models\n",
    "random_coref_model = RandomModel(SPACY_MODEL)\n",
    "random_proref_model = PronounResolutionModel(random_coref_model)\n",
    "\n",
    "token_distance_coref_model = TokenDistanceModel(SPACY_MODEL)\n",
    "token_distance_proref_model = PronounResolutionModel(token_distance_coref_model)\n",
    "\n",
    "syntactic_distance_coref_model = StanfordSyntacticDistanceModel(STANFORD_MODEL)\n",
    "syntactic_distance_proref_model = PronounResolutionModel(syntactic_distance_coref_model, n_jobs=12)\n",
    "\n",
    "parallelism_coref_model = ParallelismModel(ALLEN_DEP_MODEL, SPACY_MODEL)\n",
    "parallelism_proref_model = PronounResolutionModel(parallelism_coref_model)\n",
    "\n",
    "url_title_coref_model = URLModel(STANFORD_MODEL)\n",
    "url_title_proref_model = PronounResolutionModel(url_title_coref_model, n_jobs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Featurize train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH_TO_TRAIN, sep='\\t')\n",
    "train_df.columns = map(lambda x: x.lower().replace('-', '_'), train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 34.26it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 43.85it/s]\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   6 out of   9 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of   9 | elapsed:    1.0s finished\n",
      "100%|██████████| 9/9 [00:00<00:00, 56.48it/s]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.74it/s]\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   6 out of   9 | elapsed:    0.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of   9 | elapsed:    1.0s finished\n",
      "100%|██████████| 9/9 [00:00<00:00, 52.09it/s]\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   6 out of   9 | elapsed:    0.6s remaining:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of   9 | elapsed:    1.0s finished\n",
      "100%|██████████| 9/9 [00:04<00:00,  2.03it/s]\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   6 out of   9 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done   9 out of   9 | elapsed:    1.7s finished\n",
      "100%|██████████| 9/9 [00:00<00:00, 51.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 26s, sys: 1.06 s, total: 5min 27s\n",
      "Wall time: 13.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creates sieve pipeline of heuristic models, applying each new heuristic with appropriate backoff models\n",
    "# Multi pass sieve - order of models provided as input is important\n",
    "#    - left to right: recall increases\n",
    "#    - right to left: precision increases\n",
    "preds = MultiPassSieveModel(random_proref_model).predict(train_df)\n",
    "score_df = add_to_score_view(preds, train_df, None, 'Random')\n",
    "\n",
    "preds = MultiPassSieveModel(token_distance_proref_model).predict(train_df)\n",
    "score_df = add_to_score_view(preds, train_df, score_df, 'Token Distance')\n",
    "\n",
    "preds = MultiPassSieveModel(syntactic_distance_proref_model,\n",
    "                           token_distance_proref_model).predict(train_df)\n",
    "score_df = add_to_score_view(preds, train_df, score_df, 'Syntactic Distance')\n",
    "\n",
    "preds = MultiPassSieveModel(parallelism_proref_model,\n",
    "                            syntactic_distance_proref_model,\n",
    "                           token_distance_proref_model).predict(train_df)\n",
    "score_df = add_to_score_view(preds, train_df, score_df, 'Parallelism')\n",
    "\n",
    "preds = MultiPassSieveModel(url_title_proref_model,\n",
    "                            parallelism_proref_model,\n",
    "                            syntactic_distance_proref_model,\n",
    "                           token_distance_proref_model).predict(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_df = add_to_score_view(preds, train_df, score_df, 'Parallelism+URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = pd.DataFrame(preds, columns=['gap_A', 'gap_B']).astype('uint8')\n",
    "# y_pred_train['gap_NEITHER'] = 1 - y_pred_train['gap_A'] - y_pred_train['gap_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train.to_csv(PATH_OUT_TRAIN_FEAT, index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Featurize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(PATH_TO_TEST, sep='\\t')\n",
    "test_df.columns = map(lambda x: x.lower().replace('-', '_'), test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:   22.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08ecf48b1227bbf0166c60d47642f349, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=12)]: Done 1776 tasks      | elapsed:  1.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262b1f9572748fadaee4adab228604fb, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 2426 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=12)]: Done 3176 tasks      | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421c24e4bbea5be2eba1ae7ea8eca67a, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 4026 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=12)]: Done 4976 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=12)]: Done 6026 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8252a7df396e6a6800eaf4dc829e20e1, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 7176 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=12)]: Done 8426 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=12)]: Done 9776 tasks      | elapsed:  7.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9c3a3a20d16502e4f621ef84676b5ce, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 11226 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=12)]: Done 12359 out of 12359 | elapsed:  9.9min finished\n",
      "  1%|          | 70/12359 [00:26<1:11:04,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 364/12359 [02:17<1:17:04,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 807/12359 [05:07<1:19:58,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1205/12359 [07:35<1:06:38,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1297/12359 [08:08<1:09:35,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1353/12359 [08:31<1:15:06,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1575/12359 [09:58<1:01:27,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1618/12359 [10:14<1:18:17,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1640/12359 [10:23<1:04:56,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1674/12359 [10:37<1:08:26,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1851/12359 [11:43<1:13:39,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2263/12359 [14:21<1:01:57,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2448/12359 [15:33<1:04:47,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 3000/12359 [19:04<56:20,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 3407/12359 [21:37<53:24,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 3442/12359 [21:51<1:06:16,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4086/12359 [25:56<54:07,  2.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 5088/12359 [32:19<45:47,  2.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 5282/12359 [33:38<46:11,  2.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 5886/12359 [37:29<38:28,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 6728/12359 [43:02<38:37,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 6805/12359 [43:32<32:39,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 6941/12359 [44:24<33:23,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 7031/12359 [44:57<35:47,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7130/12359 [45:34<42:30,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7210/12359 [46:10<41:27,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 7286/12359 [46:42<33:05,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 7344/12359 [47:05<36:17,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 7625/12359 [48:49<24:50,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 8174/12359 [52:18<33:06,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 9557/12359 [1:01:38<24:08,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 9596/12359 [1:01:53<16:46,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 10359/12359 [1:06:53<14:05,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 10618/12359 [1:08:37<11:39,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 10654/12359 [1:08:50<14:41,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 10904/12359 [1:10:30<10:12,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 11000/12359 [1:11:10<10:36,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 11015/12359 [1:11:16<08:47,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 11264/12359 [1:12:53<06:10,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 11296/12359 [1:13:04<06:02,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11376/12359 [1:13:35<05:55,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency parse and tokenizer tokens dont match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12359/12359 [1:20:15<00:00,  2.49it/s]\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:   18.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08ecf48b1227bbf0166c60d47642f349, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=12)]: Done 1226 tasks      | elapsed:   57.2s\n",
      "[Parallel(n_jobs=12)]: Done 1776 tasks      | elapsed:  1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262b1f9572748fadaee4adab228604fb, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 2426 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=12)]: Done 3176 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421c24e4bbea5be2eba1ae7ea8eca67a, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 4026 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=12)]: Done 4976 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=12)]: Done 6026 tasks      | elapsed:  4.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8252a7df396e6a6800eaf4dc829e20e1, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 7176 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=12)]: Done 8426 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=12)]: Done 9776 tasks      | elapsed:  7.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9c3a3a20d16502e4f621ef84676b5ce, Tokens in parse tree and input sentence don't match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 11226 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=12)]: Done 12359 out of 12359 | elapsed:  9.7min finished\n",
      "100%|██████████| 12359/12359 [03:33<00:00, 57.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2d 1h 53min 2s, sys: 2min 16s, total: 2d 1h 55min 18s\n",
      "Wall time: 1h 43min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gap_test_preds = MultiPassSieveModel(url_title_proref_model,\n",
    "                            parallelism_proref_model,\n",
    "                            syntactic_distance_proref_model,\n",
    "                           token_distance_proref_model).predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = pd.DataFrame(gap_test_preds, columns=['gap_A', 'gap_B']).astype('uint8')\n",
    "# y_pred_test['gap_NEITHER'] = 1 - y_pred_test['gap_A'] - y_pred_test['gap_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test.to_csv(PATH_OUT_TEST_FEAT, index=None, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
